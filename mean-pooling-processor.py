#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç°¡åŒ–ç‰ˆæœ¬ï¼šç›´æ¥å¾ input_ids è¨ˆç®— mean poolingï¼ˆåŸºæ–¼ token embeddingsï¼‰
è®€å– sentences_embeddings_{00-15}.csv ä¸¦ç”Ÿæˆå°æ‡‰çš„ pooled_embeddings_{00-15}.csv
"""

import os
import csv
import pandas as pd
import torch
import numpy as np
from transformers import BertTokenizerFast, AutoModel
from pathlib import Path
import argparse
from tqdm import tqdm
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import ast
import warnings
warnings.filterwarnings('ignore')

# å®šç¾©è³‡æ–™å¤¾è·¯å¾‘
DATA_DIR = "/Users/hochienhuang/JRAR/projects/Disputability/data/Dataset"

class SimpleMeanPoolingProcessor:
    def __init__(self):
        """
        åˆå§‹åŒ–ç°¡åŒ– Mean Pooling è™•ç†å™¨
        ä½¿ç”¨ BERT embeddings é€²è¡Œ mean pooling
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print("ğŸ”„ æ­£åœ¨è¼‰å…¥ BERT ä¸­æ–‡æ¨¡å‹...")
        # è¼‰å…¥ tokenizer å’Œ model
        self.tokenizer = BertTokenizerFast.from_pretrained('ckiplab/bert-base-chinese')
        self.model = AutoModel.from_pretrained('ckiplab/bert-base-chinese')
        self.model.to(self.device)
        self.model.eval()
        
        # ç²å– embedding layer
        self.embedding_layer = self.model.embeddings.word_embeddings
        
        # ç²å–æ¨¡å‹çš„éš±è—å±¤ç¶­åº¦
        self.hidden_size = self.model.config.hidden_size
        self.max_length = 512  # BERT æœ€å¤§åºåˆ—é•·åº¦
        
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        print(f"ğŸ“ è¨­å‚™: {self.device}")
        print(f"ğŸ“ åµŒå…¥ç¶­åº¦: {self.hidden_size}")
    
    def simple_mean_pooling(self, input_ids, attention_mask):
        """
        ä½¿ç”¨ token embeddings é€²è¡Œç°¡å–®çš„ mean pooling
        
        Args:
            input_ids (list): BERT token IDs
            attention_mask (list): æ³¨æ„åŠ›é®ç½©
            
        Returns:
            np.ndarray: Mean pooled åµŒå…¥å‘é‡ (ç¶­åº¦: hidden_size)
        """
        try:
            # è½‰æ›ç‚º tensor
            input_ids_tensor = torch.tensor(input_ids, device=self.device)
            attention_mask_tensor = torch.tensor(attention_mask, device=self.device)
            
            # ç²å– token embeddings
            with torch.no_grad():
                token_embeddings = self.embedding_layer(input_ids_tensor)  # [seq_len, hidden_size]
                
                # æ‡‰ç”¨ attention mask
                masked_embeddings = token_embeddings * attention_mask_tensor.unsqueeze(-1)  # [seq_len, hidden_size]
                
                # è¨ˆç®—æœ‰æ•ˆ token æ•¸é‡
                valid_token_count = attention_mask_tensor.sum()
                
                if valid_token_count > 0:
                    # Mean pooling
                    mean_pooled = masked_embeddings.sum(dim=0) / valid_token_count  # [hidden_size]
                else:
                    # å¦‚æœæ²’æœ‰æœ‰æ•ˆ tokenï¼Œè¿”å›é›¶å‘é‡
                    mean_pooled = torch.zeros(self.hidden_size, device=self.device)
                
                # è½‰æ›ç‚º numpy array
                embedding = mean_pooled.cpu().numpy()
                
                return embedding
                
        except Exception as e:
            print(f"âš ï¸ Mean pooling è™•ç†éŒ¯èª¤: {e}")
            # è¿”å›é›¶å‘é‡ä½œç‚ºå‚™ç”¨
            return np.zeros(self.hidden_size, dtype=np.float32)
    
    def batch_simple_mean_pooling(self, input_ids_batch, attention_mask_batch):
        """
        æ‰¹é‡è™•ç† token embeddings mean pooling
        
        Args:
            input_ids_batch (list): æ‰¹é‡ BERT token IDs
            attention_mask_batch (list): æ‰¹é‡æ³¨æ„åŠ›é®ç½©
            
        Returns:
            np.ndarray: æ‰¹é‡ Mean pooled åµŒå…¥å‘é‡
        """
        try:
            embeddings = []
            for input_ids, attention_mask in zip(input_ids_batch, attention_mask_batch):
                embedding = self.simple_mean_pooling(input_ids, attention_mask)
                embeddings.append(embedding)
            
            return np.array(embeddings)
            
        except Exception as e:
            print(f"âš ï¸ æ‰¹é‡ Mean pooling è™•ç†éŒ¯èª¤: {e}")
            batch_size = len(input_ids_batch)
            return np.zeros((batch_size, self.hidden_size), dtype=np.float32)
    
    def parse_list_string(self, list_str):
        """
        è§£æå­—ä¸²å½¢å¼çš„ list
        
        Args:
            list_str (str): å­—ä¸²å½¢å¼çš„ list, ä¾‹å¦‚ "[1, 2, 3]"
            
        Returns:
            list: è§£æå¾Œçš„ list
        """
        try:
            return ast.literal_eval(list_str)
        except Exception as e:
            print(f"âš ï¸ è§£æ list å­—ä¸²éŒ¯èª¤: {e}")
            return []
    
    def check_progress(self, output_file):
        """
        æª¢æŸ¥å·²è™•ç†çš„é€²åº¦
        
        Args:
            output_file (str): è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
            int: å·²è™•ç†çš„è¡Œæ•¸
        """
        if not os.path.exists(output_file):
            return 0
        
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                # è¨ˆç®—è¡Œæ•¸ï¼ˆæ¸›å»æ¨™é¡Œè¡Œï¼‰
                line_count = sum(1 for line in f) - 1
                return max(0, line_count)
        except Exception as e:
            print(f"âš ï¸ æª¢æŸ¥é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0
    
    def process_file(self, input_file, output_file, batch_size=100, resume=True):
        """
        è™•ç†å–®å€‹ CSV æª”æ¡ˆï¼Œé€²è¡Œç°¡åŒ– mean pooling
        
        Args:
            input_file (str): è¼¸å…¥æª”æ¡ˆè·¯å¾‘
            output_file (str): è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            batch_size (int): æ‰¹é‡å¤§å°
            resume (bool): æ˜¯å¦å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ
        """
        if not os.path.exists(input_file):
            print(f"âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: {input_file}")
            return
        
        # æª¢æŸ¥é€²åº¦
        start_row = 0
        if resume:
            start_row = self.check_progress(output_file)
            if start_row > 0:
                print(f"ğŸ“ å¾ç¬¬ {start_row + 1} è¡Œç¹¼çºŒè™•ç† {input_file}")
        
        # è®€å–è¼¸å…¥æª”æ¡ˆ
        print(f"ğŸ“– æ­£åœ¨è®€å– {input_file}...")
        try:
            df = pd.read_csv(input_file)
            total_rows = len(df)
            print(f"   ç¸½å…± {total_rows:,} è¡Œæ•¸æ“š")
            
            if start_row >= total_rows:
                print(f"âœ… {input_file} å·²å®Œå…¨è™•ç†å®Œæˆ")
                return
                
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆéŒ¯èª¤: {e}")
            return
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = ['æª”å', 'å¥å­ç·¨è™Ÿ', 'DISPUTABILITY', 'input_ids', 'attention_mask']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
            return
        
        # æº–å‚™è¼¸å‡ºæª”æ¡ˆ
        file_mode = 'a' if start_row > 0 else 'w'
        write_header = start_row == 0
        
        print(f"ğŸ’¾ é–‹å§‹æ‰¹é‡è™•ç†ä¸¦å¯«å…¥ {output_file}...")
        
        try:
            with open(output_file, file_mode, newline='', encoding='utf-8') as csvfile:
                # å®šç¾© CSV æ¬„ä½ - ä½¿ç”¨å–®ä¸€ embedding æ¬„ä½
                fieldnames = [
                    'æª”å', 'å¥å­ç·¨è™Ÿ', 'DISPUTABILITY', 'embedding'
                ]
                
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                # å¯«å…¥æ¨™é¡Œè¡Œ
                if write_header:
                    writer.writeheader()
                
                # è™•ç†æ•¸æ“š
                processed_count = 0
                error_count = 0
                
                # è¨ˆç®—æ‰¹é‡æ•¸é‡
                remaining_rows = total_rows - start_row
                num_batches = (remaining_rows + batch_size - 1) // batch_size
                
                # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
                with tqdm(total=remaining_rows, 
                         desc=f"ç°¡åŒ–è™•ç† {os.path.basename(input_file)}", 
                         initial=0) as pbar:
                    
                    for batch_idx in range(num_batches):
                        batch_start = start_row + batch_idx * batch_size
                        batch_end = min(batch_start + batch_size, total_rows)
                        
                        # æº–å‚™æ‰¹é‡æ•¸æ“š
                        batch_input_ids = []
                        batch_attention_masks = []
                        batch_metadata = []
                        
                        valid_indices = []
                        
                        for idx in range(batch_start, batch_end):
                            try:
                                row = df.iloc[idx]
                                
                                # è§£æ input_ids å’Œ attention_mask
                                input_ids = self.parse_list_string(row['input_ids'])
                                attention_mask = self.parse_list_string(row['attention_mask'])
                                
                                # æª¢æŸ¥é•·åº¦
                                if len(input_ids) != self.max_length or len(attention_mask) != self.max_length:
                                    error_count += 1
                                    if error_count <= 5:
                                        print(f"âš ï¸ ç¬¬ {idx + 1} è¡Œé•·åº¦ä¸ç¬¦: input_ids={len(input_ids)}, attention_mask={len(attention_mask)}")
                                    continue
                                
                                batch_input_ids.append(input_ids)
                                batch_attention_masks.append(attention_mask)
                                batch_metadata.append({
                                    'æª”å': row['æª”å'],
                                    'å¥å­ç·¨è™Ÿ': row['å¥å­ç·¨è™Ÿ'],
                                    'DISPUTABILITY': row['DISPUTABILITY']
                                })
                                valid_indices.append(idx)
                                
                            except Exception as e:
                                error_count += 1
                                if error_count <= 5:
                                    print(f"âš ï¸ è™•ç†ç¬¬ {idx + 1} è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        
                        # å¦‚æœæ‰¹é‡ä¸­æœ‰æœ‰æ•ˆæ•¸æ“šï¼Œé€²è¡Œè™•ç†
                        if batch_input_ids:
                            try:
                                # é€²è¡Œæ‰¹é‡ç°¡åŒ– mean pooling
                                embeddings = self.batch_simple_mean_pooling(batch_input_ids, batch_attention_masks)
                                
                                # å¯«å…¥çµæœ
                                for i, (embedding, metadata) in enumerate(zip(embeddings, batch_metadata)):
                                    output_row = metadata.copy()
                                    
                                    # å°‡åµŒå…¥å‘é‡è½‰æ›ç‚º list å­—ä¸²æ ¼å¼
                                    embedding_list = embedding.tolist()
                                    output_row['embedding'] = str(embedding_list)
                                    
                                    writer.writerow(output_row)
                                    processed_count += 1
                                
                                # å¼·åˆ¶åˆ·æ–°
                                csvfile.flush()
                                
                            except Exception as e:
                                error_count += len(batch_input_ids)
                                print(f"âš ï¸ æ‰¹é‡è™•ç†éŒ¯èª¤: {e}")
                        
                        # æ›´æ–°é€²åº¦æ¢
                        pbar.update(batch_end - batch_start)
                
                print(f"âœ… å®Œæˆè™•ç† {input_file}")
                print(f"   æˆåŠŸè™•ç†: {processed_count:,} è¡Œ")
                print(f"   éŒ¯èª¤æ•¸é‡: {error_count}")
                
        except Exception as e:
            print(f"âŒ å¯«å…¥æª”æ¡ˆéŒ¯èª¤: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å° BERT input_ids é€²è¡Œç°¡åŒ– mean pooling')
    parser.add_argument('--file_id', type=str, help='è™•ç†ç‰¹å®šæª”æ¡ˆ ID (00-15)')
    parser.add_argument('--batch_size', type=int, default=100, help='æ‰¹é‡å¤§å° (é è¨­: 100)')
    parser.add_argument('--no_resume', action='store_true', help='ä¸å¾ä¸­æ–·è™•ç¹¼çºŒï¼Œé‡æ–°é–‹å§‹')
    
    args = parser.parse_args()
    
    print("ğŸš€ ç°¡åŒ– Mean Pooling è™•ç†å™¨")
    print("=" * 60)
    
    # åˆå§‹åŒ–è™•ç†å™¨
    processor = SimpleMeanPoolingProcessor()
    
    # ç¢ºå®šè¦è™•ç†çš„æª”æ¡ˆ
    if args.file_id:
        # è™•ç†å–®å€‹æª”æ¡ˆ
        file_ids = [args.file_id]
    else:
        # è™•ç†æ‰€æœ‰æª”æ¡ˆ (00-15)
        file_ids = [f"{i:02d}" for i in range(16)]
    
    print(f"ğŸ“‚ æº–å‚™è™•ç†æª”æ¡ˆ: {file_ids}")
    print(f"ğŸ”§ è¨­å®š:")
    print(f"   æ‰¹é‡å¤§å°: {args.batch_size}")
    print(f"   ç¹¼çºŒæ¨¡å¼: {'å¦' if args.no_resume else 'æ˜¯'}")
    print(f"   è¼¸å‡ºç¶­åº¦: {processor.hidden_size}")
    print("-" * 60)
    
    # è™•ç†æª”æ¡ˆ
    start_time = time.time()
    
    for file_id in file_ids:
        print(f"\nğŸ“„ è™•ç†æª”æ¡ˆ: {file_id}")
        
        input_file = os.path.join(DATA_DIR, f"sentences_embeddings_{file_id}.csv")
        output_file = os.path.join(DATA_DIR, f"simple_pooled_embeddings_{file_id}.csv")
        
        if not os.path.exists(input_file):
            print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨ï¼Œè·³é: {input_file}")
            continue
        
        processor.process_file(
            input_file=input_file,
            output_file=output_file,
            batch_size=args.batch_size,
            resume=not args.no_resume
        )
    
    total_duration = time.time() - start_time
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ å…¨éƒ¨è™•ç†å®Œæˆï¼")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {DATA_DIR}/simple_pooled_embeddings_*.csv")
    print(f"ğŸ“ æ¯å€‹å¥å­çš„åµŒå…¥ç¶­åº¦: {processor.hidden_size}")
    print(f"â±ï¸ ç¸½è€—æ™‚: {total_duration:.1f} ç§’")

if __name__ == "__main__":
    main()
