#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°‡åˆ¤æ±ºæ›¸å¥å­è½‰æ›ç‚º BERT åµŒå…¥
è®€å– judgments_sentences_{00-15}.csv ä¸¦ç”Ÿæˆå°æ‡‰çš„ sentences_embeddings_{00-15}.csv
"""

import os
import csv
import pandas as pd
import torch
from transformers import (
    BertTokenizerFast,
    AutoModel,
)
from pathlib import Path
import argparse
from tqdm import tqdm
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# å®šç¾©è³‡æ–™å¤¾è·¯å¾‘
DATA_DIR = "/Users/hochienhuang/JRAR/projects/Disputability/data/Dataset"

class SentenceEmbeddingProcessor:
    def __init__(self, max_length=512):
        """
        åˆå§‹åŒ–è™•ç†å™¨
        
        Args:
            max_length (int): BERT è¼¸å…¥çš„æœ€å¤§é•·åº¦
        """
        self.max_length = max_length
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ç‚ºæ¯å€‹ç·šç¨‹å‰µå»ºç¨ç«‹çš„ tokenizer å’Œ model
        self._local = threading.local()
        
        print("ğŸ”„ æ­£åœ¨è¼‰å…¥ BERT ä¸­æ–‡æ¨¡å‹å’Œåˆ†è©å™¨...")
        # ä¸»ç·šç¨‹ä¸­é è¼‰å…¥ä¸€æ¬¡
        self.tokenizer = BertTokenizerFast.from_pretrained('ckiplab/bert-base-chinese')
        self.model = AutoModel.from_pretrained('ckiplab/bert-base-chinese')
        self.model.to(self.device)
        self.model.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
        
        # ç²å–æ¨¡å‹çš„éš±è—å±¤ç¶­åº¦
        self.hidden_size = self.model.config.hidden_size
        
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
        print(f"ğŸ“ è¨­å‚™: {self.device}")
        print(f"ğŸ“ åµŒå…¥ç¶­åº¦: {self.hidden_size}")
    
    def get_tokenizer(self):
        """ç²å–ç·šç¨‹æœ¬åœ°çš„ tokenizer"""
        if not hasattr(self._local, 'tokenizer'):
            self._local.tokenizer = BertTokenizerFast.from_pretrained('ckiplab/bert-base-chinese')
        return self._local.tokenizer
    
    def get_model(self):
        """ç²å–ç·šç¨‹æœ¬åœ°çš„ model"""
        if not hasattr(self._local, 'model'):
            self._local.model = AutoModel.from_pretrained('ckiplab/bert-base-chinese')
            self._local.model.to(self.device)
            self._local.model.eval()
        return self._local.model
    
    def process_sentence(self, sentence):
        """
        å°‡å¥å­è½‰æ›ç‚ºå›ºå®šé•·åº¦çš„ input_ids å’Œ attention_mask
        
        Args:
            sentence (str): è¼¸å…¥å¥å­
            
        Returns:
            tuple: (input_ids_list, attention_mask_list)
        """
        # ä½¿ç”¨ç·šç¨‹æœ¬åœ°çš„ tokenizer
        tokenizer = self.get_tokenizer()
        
        # ä½¿ç”¨ BERT åˆ†è©å™¨é€²è¡Œç·¨ç¢¼
        encoded = tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        # è½‰æ›ç‚º Python list (æ–¹ä¾¿å­˜å…¥ CSV)
        input_ids = encoded['input_ids'][0].tolist()
        attention_mask = encoded['attention_mask'][0].tolist()
        
        return input_ids, attention_mask
    
    def check_progress(self, output_file):
        """
        æª¢æŸ¥å·²è™•ç†çš„é€²åº¦
        
        Args:
            output_file (str): è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
            int: å·²è™•ç†çš„è¡Œæ•¸
        """
        if not os.path.exists(output_file):
            return 0
        
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                # è¨ˆç®—è¡Œæ•¸ï¼ˆæ¸›å»æ¨™é¡Œè¡Œï¼‰
                line_count = sum(1 for line in f) - 1
                return max(0, line_count)
        except Exception as e:
            print(f"âš ï¸ æª¢æŸ¥é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0
    
    def process_file(self, input_file, output_file, resume=True):
        """
        è™•ç†å–®å€‹ CSV æª”æ¡ˆ
        
        Args:
            input_file (str): è¼¸å…¥æª”æ¡ˆè·¯å¾‘
            output_file (str): è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            resume (bool): æ˜¯å¦å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ
        """
        if not os.path.exists(input_file):
            print(f"âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: {input_file}")
            return
        
        # æª¢æŸ¥é€²åº¦
        start_row = 0
        if resume:
            start_row = self.check_progress(output_file)
            if start_row > 0:
                print(f"ğŸ“ å¾ç¬¬ {start_row + 1} è¡Œç¹¼çºŒè™•ç† {input_file}")
        
        # è®€å–è¼¸å…¥æª”æ¡ˆ
        print(f"ğŸ“– æ­£åœ¨è®€å– {input_file}...")
        try:
            df = pd.read_csv(input_file)
            total_rows = len(df)
            print(f"   ç¸½å…± {total_rows:,} è¡Œæ•¸æ“š")
            
            if start_row >= total_rows:
                print(f"âœ… {input_file} å·²å®Œå…¨è™•ç†å®Œæˆ")
                return
                
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆéŒ¯èª¤: {e}")
            return
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = ['æª”å', 'å¥å­ç·¨è™Ÿ', 'å¥å­å…§å®¹', 'DISPUTABILITY']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
            return
        
        # æº–å‚™è¼¸å‡ºæª”æ¡ˆ
        file_mode = 'a' if start_row > 0 else 'w'
        write_header = start_row == 0
        
        print(f"ğŸ’¾ é–‹å§‹è™•ç†ä¸¦å¯«å…¥ {output_file}...")
        
        try:
            with open(output_file, file_mode, newline='', encoding='utf-8') as csvfile:
                # å®šç¾© CSV æ¬„ä½
                fieldnames = [
                    'æª”å', 'å¥å­ç·¨è™Ÿ', 'DISPUTABILITY', 
                    'input_ids', 'attention_mask'
                ]
                
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                # å¯«å…¥æ¨™é¡Œè¡Œ
                if write_header:
                    writer.writeheader()
                
                # è™•ç†æ•¸æ“š
                processed_count = 0
                error_count = 0
                
                # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
                for idx in tqdm(range(start_row, total_rows), 
                              desc=f"è™•ç† {os.path.basename(input_file)}", 
                              initial=start_row, 
                              total=total_rows):
                    
                    try:
                        row = df.iloc[idx]
                        sentence = str(row['å¥å­å…§å®¹'])
                        
                        # è·³éç©ºå¥å­
                        if not sentence or sentence.strip() == '':
                            continue
                        
                        # ç”ŸæˆåµŒå…¥
                        input_ids, attention_mask = self.process_sentence(sentence)
                        
                        # å¯«å…¥çµæœ
                        writer.writerow({
                            'æª”å': row['æª”å'],
                            'å¥å­ç·¨è™Ÿ': row['å¥å­ç·¨è™Ÿ'],
                            'DISPUTABILITY': row['DISPUTABILITY'],
                            'input_ids': str(input_ids),  # è½‰æ›ç‚ºå­—ä¸²å­˜å„²
                            'attention_mask': str(attention_mask)
                        })
                        
                        processed_count += 1
                        
                        # æ¯è™•ç† 1000 è¡Œå¼·åˆ¶åˆ·æ–°
                        if processed_count % 1000 == 0:
                            csvfile.flush()
                            
                    except Exception as e:
                        error_count += 1
                        if error_count <= 5:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤
                            print(f"âš ï¸ è™•ç†ç¬¬ {idx + 1} è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
                print(f"âœ… å®Œæˆè™•ç† {input_file}")
                print(f"   æˆåŠŸè™•ç†: {processed_count:,} è¡Œ")
                print(f"   éŒ¯èª¤æ•¸é‡: {error_count}")
                
        except Exception as e:
            print(f"âŒ å¯«å…¥æª”æ¡ˆéŒ¯èª¤: {e}")
            
    def process_file_wrapper(self, file_id, resume=True):
        """
        æª”æ¡ˆè™•ç†åŒ…è£å‡½æ•¸ï¼Œç”¨æ–¼å¤šåŸ·è¡Œç·’
        
        Args:
            file_id (str): æª”æ¡ˆ ID (00-15)
            resume (bool): æ˜¯å¦å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒ
            
        Returns:
            dict: è™•ç†çµæœ
        """
        input_file = os.path.join(DATA_DIR, f"judgments_sentences_{file_id}.csv")
        output_file = os.path.join(DATA_DIR, f"sentences_embeddings_{file_id}.csv")
        
        start_time = time.time()
        
        try:
            if not os.path.exists(input_file):
                return {
                    'file_id': file_id,
                    'status': 'skipped',
                    'message': f'æª”æ¡ˆä¸å­˜åœ¨: {input_file}',
                    'duration': 0
                }
            
            # åŸ·è¡Œè™•ç†
            self.process_file(input_file, output_file, resume)
            
            duration = time.time() - start_time
            return {
                'file_id': file_id,
                'status': 'success',
                'message': f'è™•ç†å®Œæˆ',
                'duration': duration
            }
            
        except Exception as e:
            duration = time.time() - start_time
            return {
                'file_id': file_id,
                'status': 'error',
                'message': f'éŒ¯èª¤: {str(e)}',
                'duration': duration
            }

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='å°‡åˆ¤æ±ºæ›¸å¥å­è½‰æ›ç‚º BERT åµŒå…¥')
    parser.add_argument('--file_id', type=str, help='è™•ç†ç‰¹å®šæª”æ¡ˆ ID (00-15)')
    parser.add_argument('--max_length', type=int, default=512, help='BERT è¼¸å…¥æœ€å¤§é•·åº¦')
    parser.add_argument('--no_resume', action='store_true', help='ä¸å¾ä¸­æ–·è™•ç¹¼çºŒï¼Œé‡æ–°é–‹å§‹')
    parser.add_argument('--threads', type=int, default=4, help='åŸ·è¡Œç·’æ•¸é‡ (é è¨­: 4)')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¤šåŸ·è¡Œç·’å¥å­åµŒå…¥è™•ç†å™¨")
    print("=" * 60)
    
    # åˆå§‹åŒ–è™•ç†å™¨
    processor = SentenceEmbeddingProcessor(max_length=args.max_length)
    
    # ç¢ºå®šè¦è™•ç†çš„æª”æ¡ˆ
    if args.file_id:
        # è™•ç†å–®å€‹æª”æ¡ˆ
        file_ids = [args.file_id]
    else:
        # è™•ç†æ‰€æœ‰æª”æ¡ˆ (00-15)
        file_ids = [f"{i:02d}" for i in range(16)]
    
    print(f"ğŸ“‚ æº–å‚™è™•ç†æª”æ¡ˆ: {file_ids}")
    print(f"ğŸ”§ è¨­å®š:")
    print(f"   æœ€å¤§é•·åº¦: {args.max_length}")
    print(f"   åŸ·è¡Œç·’æ•¸é‡: {args.threads}")
    print(f"   ç¹¼çºŒæ¨¡å¼: {'å¦' if args.no_resume else 'æ˜¯'}")
    print("-" * 60)
    
    # è™•ç†æª”æ¡ˆ
    total_files = len(file_ids)
    start_time = time.time()
    
    if args.file_id:
        # å–®æª”æ¡ˆè™•ç†ï¼ˆä¸ä½¿ç”¨å¤šåŸ·è¡Œç·’ï¼‰
        print(f"\nğŸ“„ è™•ç†å–®å€‹æª”æ¡ˆ: {args.file_id}")
        
        input_file = os.path.join(DATA_DIR, f"judgments_sentences_{args.file_id}.csv")
        output_file = os.path.join(DATA_DIR, f"sentences_embeddings_{args.file_id}.csv")
        
        if not os.path.exists(input_file):
            print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {input_file}")
            return
        
        processor.process_file(
            input_file=input_file,
            output_file=output_file,
            resume=not args.no_resume
        )
    else:
        # å¤šæª”æ¡ˆå¤šåŸ·è¡Œç·’è™•ç†
        print(f"\nğŸ§µ ä½¿ç”¨ {args.threads} å€‹åŸ·è¡Œç·’åŒæ™‚è™•ç† {total_files} å€‹æª”æ¡ˆ")
        
        # å»ºç«‹åŸ·è¡Œç·’æ± 
        with ThreadPoolExecutor(max_workers=args.threads) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_file = {
                executor.submit(processor.process_file_wrapper, file_id, not args.no_resume): file_id
                for file_id in file_ids
                if os.path.exists(os.path.join(DATA_DIR, f"judgments_sentences_{file_id}.csv"))
            }
            
            # è™•ç†å®Œæˆçš„ä»»å‹™
            completed = 0
            results = []
            
            for future in as_completed(future_to_file):
                file_id = future_to_file[future]
                completed += 1
                
                try:
                    result = future.result()
                    results.append(result)
                    
                    status_icon = "âœ…" if result['status'] == 'success' else "âš ï¸" if result['status'] == 'skipped' else "âŒ"
                    duration_str = f"{result['duration']:.1f}s" if result['duration'] > 0 else "0s"
                    
                    print(f"{status_icon} [{completed:2d}/{len(future_to_file):2d}] {file_id}: {result['message']} ({duration_str})")
                    
                except Exception as exc:
                    print(f"âŒ [{completed:2d}/{len(future_to_file):2d}] {file_id}: åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {exc}")
                    results.append({
                        'file_id': file_id,
                        'status': 'error',
                        'message': f'åŸ·è¡ŒéŒ¯èª¤: {str(exc)}',
                        'duration': 0
                    })
        
        # é¡¯ç¤ºç¸½çµ
        total_duration = time.time() - start_time
        success_count = sum(1 for r in results if r['status'] == 'success')
        error_count = sum(1 for r in results if r['status'] == 'error')
        skipped_count = sum(1 for r in results if r['status'] == 'skipped')
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š è™•ç†ç¸½çµ:")
        print(f"   âœ… æˆåŠŸ: {success_count} å€‹æª”æ¡ˆ")
        print(f"   âŒ éŒ¯èª¤: {error_count} å€‹æª”æ¡ˆ")
        print(f"   âš ï¸ è·³é: {skipped_count} å€‹æª”æ¡ˆ")
        print(f"   â±ï¸ ç¸½è€—æ™‚: {total_duration:.1f} ç§’")
        print(f"   ğŸš€ å¹³å‡æ¯æª”æ¡ˆ: {total_duration/len(results):.1f} ç§’" if results else "")
    
    print(f"\nğŸ¯ å…¨éƒ¨è™•ç†å®Œæˆï¼")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {DATA_DIR}/sentences_embeddings_*.csv")

if __name__ == "__main__":
    main()
