#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾åˆ¤æ±ºæ›¸ JSON æª”æ¡ˆä¸­æå–ä¸»æ–‡éƒ¨åˆ†ä¸¦è½‰æ›ç‚º CSV æ ¼å¼
ä½¿ç”¨å¤šç·šç¨‹è™•ç†ï¼Œè¼¸å‡º16å€‹åˆ†å‰²æª”æ¡ˆ
"""

import os
import re
import json
import csv
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import math

def extract_main_text(jfull_content):
    """
    å¾ JFULL å…§å®¹ä¸­æå–ä¸»æ–‡éƒ¨åˆ†ï¼ˆåœ¨"ä¸»æ–‡"å’Œ"ä¸­è¯æ°‘åœ‹...å¹´æœˆæ—¥"ä¹‹é–“çš„æ–‡å­—ï¼‰
    
    Args:
        jfull_content (str): JFULL å±¬æ€§çš„å®Œæ•´å…§å®¹
        
    Returns:
        str: æå–çš„ä¸»æ–‡å…§å®¹ï¼Œå¦‚æœæ²’æœ‰æ‰¾åˆ°å‰‡è¿”å›ç©ºå­—ä¸²
    """
    # å®šç¾©è¦åŒ¹é…çš„æ­£å‰‡è¡¨é”å¼æ¨¡å¼
    pattern1 = r'^\s*ä¸»\s*æ–‡\s*$'  # åŒ¹é… "ä¸»æ–‡"
    pattern2 = r'^\s*ä¸­\s*è¯\s*æ°‘\s*åœ‹.*å¹´.*æœˆ.*æ—¥\s*$'  # åŒ¹é… "ä¸­è¯æ°‘åœ‹...å¹´...æœˆ...æ—¥"
    
    # ç·¨è­¯æ­£å‰‡è¡¨é”å¼ï¼Œä½¿ç”¨ MULTILINE æ¨¡å¼
    regex1 = re.compile(pattern1, re.MULTILINE)
    regex2 = re.compile(pattern2, re.MULTILINE)
    
    # æ‰¾åˆ°ä¸»æ–‡çš„ä½ç½®
    main_match = regex1.search(jfull_content)
    if not main_match:
        return ""
    
    # æ‰¾åˆ°æ—¥æœŸçš„ä½ç½®
    date_match = regex2.search(jfull_content)
    if not date_match:
        return ""
    
    # ç¢ºä¿ä¸»æ–‡åœ¨æ—¥æœŸä¹‹å‰
    if main_match.end() >= date_match.start():
        return ""
    
    # æå–ä¸»æ–‡éƒ¨åˆ†ï¼ˆå¾ä¸»æ–‡çµæŸåˆ°æ—¥æœŸé–‹å§‹ä¹‹é–“çš„å…§å®¹ï¼‰
    main_text = jfull_content[main_match.end():date_match.start()]
    
    return main_text.strip()

def clean_and_split_text(text):
    """
    æ¸…ç†æ–‡å­—ä¸¦æŒ‰å¥è™Ÿåˆ†å‰²æˆå¥å­
    
    Args:
        text (str): åŸå§‹æ–‡å­—
        
    Returns:
        list: å¥å­åˆ—è¡¨
    """
    if not text:
        return []
    
    # ç§»é™¤æ‰€æœ‰æ›è¡Œç¬¦è™Ÿå’Œå¤šé¤˜ç©ºæ ¼
    cleaned_text = re.sub(r'\s+', '', text)
    
    # æŒ‰å¥è™Ÿåˆ†å‰²
    sentences = cleaned_text.split('ã€‚')
    
    # éæ¿¾æ‰ç©ºå¥å­
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    
    return sentences

def process_json_files(input_dir, output_prefix, num_threads=16):
    """
    ä½¿ç”¨å¤šç·šç¨‹è™•ç†æ‰€æœ‰ JSON æª”æ¡ˆä¸¦è¼¸å‡ºå¤šå€‹ CSV æª”æ¡ˆ
    
    Args:
        input_dir (str): è¼¸å…¥ç›®éŒ„è·¯å¾‘
        output_prefix (str): è¼¸å‡º CSV æª”æ¡ˆå‰ç¶´
        num_threads (int): ç·šç¨‹æ•¸é‡ï¼Œé è¨­ç‚º16
    """
    input_path = Path(input_dir)
    
    if not input_path.exists():
        print(f"éŒ¯èª¤ï¼šç›®éŒ„ {input_path} ä¸å­˜åœ¨")
        return
    
    # ç²å–æ‰€æœ‰ JSON æª”æ¡ˆ
    json_files = list(input_path.glob("*.json"))
    
    if not json_files:
        print(f"åœ¨ {input_path} ä¸­æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ")
        return
    
    print(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
    print(f"ä½¿ç”¨ {num_threads} å€‹ç·šç¨‹è™•ç†")
    
    # åˆ†å‰²æª”æ¡ˆåˆ—è¡¨
    files_per_thread = math.ceil(len(json_files) / num_threads)
    file_chunks = []
    
    for i in range(num_threads):
        start_idx = i * files_per_thread
        end_idx = min((i + 1) * files_per_thread, len(json_files))
        if start_idx < len(json_files):
            chunk = json_files[start_idx:end_idx]
            file_chunks.append((i, chunk))
    
    print(f"åˆ†å‰²ç‚º {len(file_chunks)} å€‹å·¥ä½œå¡Š")
    
    # ä½¿ç”¨ç·šç¨‹æ± è™•ç†
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        # æäº¤æ‰€æœ‰å·¥ä½œ
        future_to_chunk = {
            executor.submit(process_file_chunk, chunk_id, files, output_prefix): chunk_id 
            for chunk_id, files in file_chunks
        }
        
        # æ”¶é›†çµæœ
        total_processed = 0
        total_sentences = 0
        total_errors = 0
        
        for future in as_completed(future_to_chunk):
            chunk_id = future_to_chunk[future]
            try:
                processed, sentences, errors = future.result()
                total_processed += processed
                total_sentences += sentences
                total_errors += errors
                print(f"ç·šç¨‹ {chunk_id} å®Œæˆï¼šè™•ç† {processed} å€‹æª”æ¡ˆï¼Œ{sentences} å€‹å¥å­")
            except Exception as e:
                print(f"ç·šç¨‹ {chunk_id} ç™¼ç”ŸéŒ¯èª¤: {e}")
                total_errors += 1
    
    print(f"\nğŸ¯ å…¨éƒ¨è™•ç†å®Œæˆï¼")
    print(f"ç¸½æª”æ¡ˆæ•¸: {len(json_files)}")
    print(f"æˆåŠŸè™•ç†: {total_processed}")
    print(f"éŒ¯èª¤æª”æ¡ˆ: {total_errors}")
    print(f"ç¸½å¥å­æ•¸: {total_sentences}")
    print(f"è¼¸å‡ºæª”æ¡ˆ: {output_prefix}_*.csv")

def process_file_chunk(chunk_id, json_files, output_prefix):
    """
    è™•ç†ä¸€å€‹æª”æ¡ˆå¡Š
    
    Args:
        chunk_id (int): å¡Šç·¨è™Ÿ
        json_files (list): è¦è™•ç†çš„ JSON æª”æ¡ˆåˆ—è¡¨
        output_prefix (str): è¼¸å‡ºæª”æ¡ˆå‰ç¶´
        
    Returns:
        tuple: (è™•ç†çš„æª”æ¡ˆæ•¸, å¥å­æ•¸, éŒ¯èª¤æ•¸)
    """
    output_file = f"{output_prefix}_{chunk_id:02d}.csv"
    
    processed_files = 0
    total_sentences = 0
    error_files = 0
    
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # å¯«å…¥æ¨™é¡Œè¡Œ
        writer.writerow(['æª”å', 'å¥å­ç·¨è™Ÿ', 'å¥å­å…§å®¹', 'DISPUTABILITY'])
        
        for i, json_file in enumerate(json_files, 1):
            try:
                # æ¯è™•ç† 100 å€‹æª”æ¡ˆé¡¯ç¤ºé€²åº¦
                if i % 100 == 0:
                    print(f"ç·šç¨‹ {chunk_id}: {i}/{len(json_files)} ({i/len(json_files)*100:.1f}%)")
                
                # è®€å– JSON æª”æ¡ˆ
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ JFULL å±¬æ€§
                if 'JFULL' not in data:
                    continue
                
                # æå–ä¸»æ–‡éƒ¨åˆ†
                main_text = extract_main_text(data['JFULL'])
                
                if not main_text:
                    continue
                
                # æ¸…ç†ä¸¦åˆ†å‰²æ–‡å­—
                sentences = clean_and_split_text(main_text)
                
                if not sentences:
                    continue
                
                # ç²å– DISPUTABILITY
                disputability = data.get('DISPUTABILITY', '')
                
                # å¯«å…¥ CSV
                filename = json_file.stem  # ä¸å«å‰¯æª”åçš„æª”å
                for sentence_num, sentence in enumerate(sentences, 1):
                    writer.writerow([filename, sentence_num, sentence, disputability])
                
                processed_files += 1
                total_sentences += len(sentences)
                
            except Exception as e:
                error_files += 1
                if error_files <= 3:  # æ¯å€‹ç·šç¨‹åªé¡¯ç¤ºå‰3å€‹éŒ¯èª¤
                    print(f"ç·šç¨‹ {chunk_id} éŒ¯èª¤ï¼šè™•ç†æª”æ¡ˆ {json_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return processed_files, total_sentences, error_files

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” åˆ¤æ±ºæ›¸ä¸»æ–‡æå–å·¥å…· (å¤šç·šç¨‹ç‰ˆæœ¬)")
    print("=" * 60)
    
    # è¨­å®šè¼¸å…¥å’Œè¼¸å‡ºè·¯å¾‘
    input_directory = "../data/filtered_judgments2"
    output_prefix = "judgments_sentences"
    num_threads = 16
    
    print(f"è¼¸å…¥ç›®éŒ„: {input_directory}")
    print(f"è¼¸å‡ºæª”æ¡ˆå‰ç¶´: {output_prefix}")
    print(f"ç·šç¨‹æ•¸é‡: {num_threads}")
    print(f"å°‡ç”¢ç”Ÿ: {output_prefix}_00.csv åˆ° {output_prefix}_{num_threads-1:02d}.csv")
    print("-" * 60)
    
    # é–‹å§‹è™•ç†
    process_json_files(input_directory, output_prefix, num_threads)
    
    print("\nâœ… å¤šç·šç¨‹è™•ç†å®Œæˆï¼")
    print(f"ğŸ“ ç”Ÿæˆçš„æª”æ¡ˆæ ¼å¼: {output_prefix}_XX.csv (å…± {num_threads} å€‹æª”æ¡ˆ)")
    print("ğŸ“ CSV æ¬„ä½: æª”å, å¥å­ç·¨è™Ÿ, å¥å­å…§å®¹, DISPUTABILITY")

if __name__ == "__main__":
    main()
